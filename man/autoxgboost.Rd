% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/autoxgboost.R
\name{autoxgboost}
\alias{autoxgboost}
\title{Fit and optimize a xgboost model.}
\usage{
autoxgboost(task, measure = NULL, control = NULL, par.set = NULL,
  max.nrounds = 10^6, early.stopping.rounds = 10L,
  early.stopping.fraction = 4/5, build.final.model = TRUE,
  design.size = 15L, impact.encoding.boundary = 10L, mbo.learner = NULL,
  nthread = NULL, tune.threshold = TRUE, validation.data = NULL)
}
\arguments{
\item{task}{[\code{\link[mlr]{Task}}]\cr
The task.}

\item{measure}{[\code{\link[mlr]{Measure}}]\cr
Performance measure. If \code{NULL} \code{\link[mlr]{getDefaultMeasure}} is used.}

\item{control}{[\code{\link[mlrMBO]{MBOControl}}]\cr
Control object for mbo. Specifies runtime behaviour.
Default is to run for 160 iterations or 1 hour, whatever happens first.}

\item{par.set}{[\code{\link[ParamHelpers]{ParamSet}}]\cr
Parameter set. Default is \code{\link{autoxgbparset}}.}

\item{max.nrounds}{[\code{integer(1)}]\cr
Maximum number of allowed iterations. Default is \code{10^6}.}

\item{early.stopping.rounds}{[\code{integer(1L}]\cr
After how many iterations without an improvement in the OOB error should be stopped?
Default is 10.}

\item{early.stopping.fraction}{[\code{numeric(1)}]\cr
What fraction of the data should be used for early stopping (i.e. as a validation set).
Default is \code{4/5}.}

\item{build.final.model}{[\code{logical(1)}]\cr
Should the best found model be fitted on the complete dataset?
Default is \code{FALSE}.}

\item{design.size}{[\code{integer(1)}]\cr
Size of the initial design. Default is \code{15L}.}

\item{impact.encoding.boundary}{[\code{integer(1)}]\cr
Defines the threshold on how factor variables are handled. Factors with more levels than the \code{"impact.encoding.boundary"} get impact encoded (see \code{\link{createImpactFeatures}}) while factor variables with less or equal levels than the \code{"impact.encoding.boundary"} get dummy encoded.
(see \code{\link[mlr]{createDummyFeatures}}). For \code{impact.encoding.boundary = 0L}, all factor variables get impact encoded while for \code{impact.encoding.boundary = Inf}, all of them get dummy encoded.
Default is \code{10L}.}

\item{mbo.learner}{[\code{\link[mlr]{Learner}}]\cr
Regression learner from mlr, which is used as a surrogate to model our fitness function.
If \code{NULL} (default), the default learner is determined as described here: \link[mlrMBO]{mbo_default_learner}.}

\item{nthread}{[integer(1)]\cr
Number of cores to use.
If \code{NULL} (default), xgboost will determine internally how many cores to use.}

\item{tune.threshold}{[logical(1)]\cr
Should thresholds be tuned? This has only an effect for classification, see \code{\link[mlr]{tuneThreshold}}.
Default is \code{TRUE}.}
}
\value{
\code{\link{AutoxgbResult}}
}
\description{
An xgboost model is optimized based on a measure (see [\code{\link[mlr]{Measure}}]).
The bounds of the parameter in which the model is optimized, are defined by \code{\link{autoxgbparset}}.
For the optimization itself bayesian optimization with \pkg{mlrMBO} is used.
Without any specification of the control object, the optimizer runs for for 80 iterations or 1 hour, whatever happens first.
Both the parameter set and the control object can be set by the user.
}
